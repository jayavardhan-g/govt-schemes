#!/usr/bin/env python3
"""
fetcher.py

- Reads seedurls.csv (header: url) from the same folder as this script.
- Renders each URL with Playwright (headless Chromium) and saves the full page HTML
  to output/raw_html/<url_key>.html
"""

import csv
import hashlib
import os
import time
from pathlib import Path
from urllib.parse import urlparse

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError


# --------- Config ----------
SCRIPT_DIR = Path(__file__).resolve().parent
SEED_CSV = SCRIPT_DIR / "seedurls.csv"
OUT_DIR = SCRIPT_DIR / "output" / "raw_html"
OUT_DIR.mkdir(parents=True, exist_ok=True)

NAV_TIMEOUT_MS = 45_000
WAIT_AFTER_NETWORK_IDLE_S = 1.0
MAX_RETRIES = 2
BROWSER_VIEWPORT = {"width": 1280, "height": 900}
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
)


def url_to_filename(url: str) -> str:
    """Create a filesystem-safe url_key for saving HTML from a URL."""
    parsed = urlparse(url)
    domain = parsed.netloc.replace(":", "_")
    path_part = parsed.path.strip("/").replace("/", "_") or "index"
    base = f"{domain}__{path_part}"
    h = hashlib.sha1(url.encode("utf-8")).hexdigest()[:8]

    url_key = "".join(
        c if c.isalnum() or c in ("-", "_", ".") else "_"
        for c in f"{base}__{h}"
    )
    return url_key


def read_seed_urls(csv_path: Path):
    if not csv_path.exists():
        print(f"seedurls.csv not found at: {csv_path}")
        return []

    urls = []
    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if "url" in row and row["url"].strip():
                urls.append(row["url"].strip())
            else:
                first = next(iter(row.values()), "").strip()
                if first:
                    urls.append(first)
    return urls


def scroll_page_slowly(page):
    page.evaluate(
        """() => {
            return new Promise(resolve => {
              const total = document.body.scrollHeight;
              let pos = 0;
              const step = Math.max(Math.floor(total / 8), 200);
              const t = setInterval(() => {
                pos = Math.min(pos + step, total);
                window.scrollTo(0, pos);
                if (pos >= total) {
                  clearInterval(t);
                  setTimeout(resolve, 300);
                }
              }, 150);
            });
        }"""
    )


def fetch_single_page(page, url: str, out_path: Path) -> bool:
    try:
        page.set_viewport_size(BROWSER_VIEWPORT)
        page.set_extra_http_headers({"User-Agent": USER_AGENT})
        page.goto(url, wait_until="networkidle", timeout=NAV_TIMEOUT_MS)

        time.sleep(WAIT_AFTER_NETWORK_IDLE_S)

        try:
            scroll_page_slowly(page)
        except Exception:
            pass

        time.sleep(0.2)

        html = page.content()
        out_path.write_text(html, encoding="utf-8")
        return True

    except PlaywrightTimeoutError as te:
        print(f"[timeout] {url} -> {te}")
        return False

    except Exception as e:
        print(f"[error] {url} -> {e}")
        return False


def main():
    urls = read_seed_urls(SEED_CSV)
    if not urls:
        print("No URLs found in seedurls.csv")
        return

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            java_script_enabled=True,
            user_agent=USER_AGENT
        )

        try:
            for idx, url in enumerate(urls, start=1):
                print(f"[{idx}/{len(urls)}] Fetching: {url}")
                url_key = url_to_filename(url)
                out_file = OUT_DIR / f"{url_key}.html"

                success = False
                attempt = 0
                while attempt <= MAX_RETRIES and not success:
                    attempt += 1
                    page = context.new_page()

                    try:
                        success = fetch_single_page(page, url, out_file)
                        if success:
                            print(f"  -> saved: {out_file}")
                        else:
                            print(f"  -> attempt {attempt} failed")
                            time.sleep(1.5 ** attempt)

                    finally:
                        try:
                            page.close()
                        except Exception:
                            pass

                if not success:
                    print(f"  -> FAILED after {MAX_RETRIES + 1} attempts: {url}")

        finally:
            context.close()
            browser.close()


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
- Scans output/raw_html/ for .html files
- Parses each HTML to extract title, eligibility description, and state
"""

import os
import csv
import re
import hashlib
from urllib.parse import urlparse
from bs4 import BeautifulSoup

# Paths
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
RAW_HTML_DIR = os.path.join(SCRIPT_DIR, "output", "raw_html")
SEED_CSV = os.path.join(SCRIPT_DIR, "seedurls.csv")
OUTPUT_PY = os.path.join(SCRIPT_DIR, "output", "sample_schemes.py")

# Keywords / heuristics
HEADING_KEYWORDS = [
    "eligibility", "eligibility criteria", "who can apply", "who is eligible",
    "conditions for eligibility", "eligible", "applicants"
]

FALLBACK_KEYWORDS = [
    "eligible", "not eligible", "income", "annual income", "age", "years", "resident",
    "citizen", "widow", "women", "household", "beneficiary", "ownership", "landholding", "student"
]

STATES = [
    "Andhra Pradesh","Arunachal Pradesh","Assam","Bihar","Chhattisgarh",
    "Goa","Gujarat","Haryana","Himachal Pradesh","Jharkhand","Karnataka",
    "Kerala","Madhya Pradesh","Maharashtra","Manipur","Meghalaya","Mizoram",
    "Nagaland","Odisha","Punjab","Rajasthan","Sikkim","Tamil Nadu",
    "Telangana","Tripura","Uttar Pradesh","Uttarakhand","West Bengal",
    "Delhi","Jammu and Kashmir","Ladakh"
]


def url_to_filename(url: str) -> str:
    parsed = urlparse(url)
    domain = parsed.netloc.replace(":", "_")
    path_part = parsed.path.strip("/").replace("/", "_") or "index"
    base = f"{domain}__{path_part}"
    h = hashlib.sha1(url.encode("utf-8")).hexdigest()[:8]
    url_key = "".join(c if c.isalnum() or c in ("-", "_", ".") else "_" for c in f"{base}__{h}")
    return url_key


def load_seed_map(csv_path=SEED_CSV):
    """
    Return dict: url_key -> url for entries in seedurls.csv.
    If seed file missing, return empty dict.
    """
    mapping = {}
    if not os.path.exists(csv_path):
        return mapping
    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # accept 'url' header or first column fallback
            url = ""
            if "url" in row and row["url"].strip():
                url = row["url"].strip()
            else:
                first = next(iter(row.values()), "").strip()
                if first:
                    url = first
            if url:
                key = url_to_filename(url)
                mapping[key] = url
    return mapping


def read_html(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return None


def clean_text(s):
    if not s:
        return ""
    s = re.sub(r"\r\n?", "\n", s)
    s = re.sub(r"\u2022|\u2023|\u25E6|\u2043|\u2219", "-", s)
    s = re.sub(r"\n\s*\n+", "\n", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()


def extract_title(soup, fallback):
    tag = soup.find("title")
    if tag and tag.get_text(strip=True):
        return tag.get_text(strip=True)
    h1 = soup.find(re.compile(r"^h[1-3]$"))
    if h1 and h1.get_text(strip=True):
        return h1.get_text(strip=True)
    return fallback


def find_heading_candidates(soup):
    heads = []
    for level in ["h1", "h2", "h3", "h4", "strong", "b"]:
        for tag in soup.find_all(level):
            txt = tag.get_text(" ", strip=True).lower()
            for kw in HEADING_KEYWORDS:
                if kw in txt:
                    heads.append(tag)
                    break
    return heads


def extract_block_after_heading(tag):
    parts = []
    for sib in tag.find_next_siblings():
        if sib.name and re.match(r"h[1-4]", sib.name, re.I):
            break
        if sib.name in ("p", "div", "ul", "ol", "table", "dl"):
            parts.append(sib.get_text("\n", strip=True))
        else:
            t = sib.get_text(" ", strip=True)
            if t:
                parts.append(t)
    return "\n".join(p for p in parts if p)


def fallback_search_for_eligibility(soup):
    candidates = []
    nodes = soup.find_all(["p", "li", "div", "td"])
    for node in nodes:
        txt = node.get_text(" ", strip=True)
        if any(k in txt.lower() for k in FALLBACK_KEYWORDS):
            candidates.append(txt)
    if candidates:
        return "\n".join(candidates[:6])
    return ""


def detect_state(text):
    if not text:
        return ""
    for s in STATES:
        if s.lower() in text.lower():
            return s
    tokens = re.findall(r"\b[A-Za-z]+\b", text)
    for tok in tokens:
        for s in STATES:
            if tok.lower() == s.split()[0].lower():
                return s
    return ""


def build_entry(title, description, state, source_url):
    return {
        "title": title if title is not None else "",
        "description": description if description is not None else "",
        "state": state if state is not None else "",
        "source_url": source_url if source_url is not None else ""
    }


def write_output_py(entries, out_path=OUTPUT_PY):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("SAMPLE_SCHEMES = [\n")
        for e in entries:
            title = repr(e["title"])
            desc = repr(e["description"])
            state = repr(e["state"])
            src = repr(e["source_url"])
            f.write(f"    {{'title': {title}, 'description': {desc}, 'state': {state}, 'source_url': {src}}},\n")
        f.write("]\n")
    print(f"Wrote {len(entries)} entries to: {out_path}")


def parse_all_html():
    # load url_key -> url map from seed list (optional)
    seed_map = load_seed_map()

    if not os.path.isdir(RAW_HTML_DIR):
        print(f"No raw HTML directory found: {RAW_HTML_DIR}")
        write_output_py([])
        return

    files = sorted(f for f in os.listdir(RAW_HTML_DIR) if f.lower().endswith(".html"))
    if not files:
        print("No .html files found under raw_html.")
        write_output_py([])
        return

    entries = []
    for fname in files:
        path = os.path.join(RAW_HTML_DIR, fname)
        html = read_html(path)
        if not html:
            print(f"[skip] could not read: {fname}")
            continue
        soup = BeautifulSoup(html, "html.parser")
        fallback_name = os.path.splitext(fname)[0]  # this is the url_key produced by fetcher
        title = extract_title(soup, fallback_name)

        # heading-first extraction
        description = ""
        heading_tags = find_heading_candidates(soup)
        if heading_tags:
            for h in heading_tags:
                blk = extract_block_after_heading(h)
                if blk and len(blk.strip()) > 20:
                    description = blk
                    break

        if not description:
            description = fallback_search_for_eligibility(soup)

        description = clean_text(description)
        state = detect_state(" ".join([title, description]))

        # try to find source_url via seed map using url_key (filename without .html)
        url_key = os.path.splitext(fname)[0]
        source_url = seed_map.get(url_key, "")

        entries.append(build_entry(title=title, description=description, state=state, source_url=source_url))
        print(f"[ok] parsed {fname} -> title: {title} (state='{state}')")

    write_output_py(entries)


if __name__ == "__main__":
    parse_all_html()
#!/usr/bin/env python3
import sys
import os
from importlib import import_module

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, SCRIPT_DIR)

def run_fetcher():
    print("\n============================")
    print(" STEP 1: Fetching HTML")
    print("============================")
    try:
        fetcher = import_module("fetcher")
    except Exception as e:
        print("Error: Could not import fetcher.py")
        print(e)
        sys.exit(1)

    fetcher.main()
    print("Fetching completed.\n")


def run_parser():
    print("\n============================")
    print(" STEP 2: Parsing HTML")
    print("============================")
    try:
        parser = import_module("parser")
    except Exception as e:
        print("Error: Could not import parser.py")
        print(e)
        sys.exit(1)

    parser.parse_all_html()
    print("Parsing completed.\n")


def main():
    run_fetcher()
    run_parser()
    print("\n============================")
    print("  DONE! Check:")
    print("  → output/raw_html/ (raw HTML files)")
    print("  → output/sample_schemes.py (final parsed output)")
    print("============================\n")


if __name__ == "__main__":
    main()
