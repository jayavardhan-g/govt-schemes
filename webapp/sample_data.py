# webapp/sample_data.py

import os
import sys
import importlib.util
from db import db
from models import Scheme, SchemeRule
from rule_parser import RuleParser

def load_scraped_schemes():
    """
    Dynamically imports SAMPLE_SCHEMES from webapp/output/sample_schemes.py
    """
    # FIX: Look for 'output' inside the current 'webapp' directory
    current_dir = os.path.dirname(os.path.abspath(__file__))
    output_file_path = os.path.join(current_dir, 'output', 'sample_schemes.py')

    if not os.path.exists(output_file_path):
        print(f"Warning: Scraped data file not found at {output_file_path}")
        print("Please run 'runner.py' inside the webapp folder first.")
        return []

    try:
        # Dynamic module import
        spec = importlib.util.spec_from_file_location("scraped_data", output_file_path)
        scraped_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(scraped_module)
        
        if hasattr(scraped_module, 'SAMPLE_SCHEMES'):
            return scraped_module.SAMPLE_SCHEMES
        else:
            print("Error: SAMPLE_SCHEMES list not found in the generated file.")
            return []
    except Exception as e:
        print(f"Error loading scraped data: {e}")
        return []

def ensure_sample_data():
    """
    1. Loads scraped data from output/sample_schemes.py
    2. Uses RuleParser to generate logic from the description text.
    3. Inserts Schemes and Rules into Postgres via SQLAlchemy.
    """
    
    # 1. Safety Check: If DB is not empty, don't overwrite.
    if Scheme.query.count() > 0:
        # print("Database already has data. Skipping seed.") 
        # Commented out print to keep logs clean on restart
        return

    print("--- Starting Data Seeding from Scraper ---")
    
    # 2. Load the raw data generated by runner.py -> parser.py
    raw_inputs = load_scraped_schemes()
    
    if not raw_inputs:
        print("No data found to insert.")
        return

    # 3. Initialize the Parser
    parser = RuleParser()
    
    count = 0
    for item in raw_inputs:
        title = item.get('title', 'Unknown Scheme')
        # The scraper puts the eligibility text in 'description'
        raw_text = item.get('description', '') 
        source_url = item.get('source_url', '')
        # The scraper might have detected a state, or we rely on RuleParser
        scraped_state = item.get('state', '')

        if not raw_text or len(raw_text) < 10:
            print(f"Skipping '{title}': Insufficient description text for parsing.")
            continue

        print(f"Processing: {title}...")

        # A. Parse the text to get the Rule JSON
        # parser.parse_text returns (rule_structure, confidence_score)
        rule_json, confidence = parser.parse_text(raw_text)

        # B. Determine State (Prioritize RuleParser result, fallback to Scraper result)
        detected_state = scraped_state
        
        # Check if RuleParser found a specific state requirement
        for rule in rule_json.get('all', []):
            if rule.get('field') == 'state' and rule.get('op') == 'in':
                if rule.get('value'):
                    # Capitalize first letter (e.g., "karnataka" -> "Karnataka")
                    detected_state = rule['value'][0].title() 
                    break

        # C. Create Scheme Object
        scheme = Scheme(
            title=title,
            description=raw_text, # Storing the full text as description
            state=detected_state, 
            source_url=source_url
        )
        
        db.session.add(scheme)
        db.session.flush() # Flush to generate the ID for the foreign key

        # D. Create SchemeRule Object
        scheme_rule = SchemeRule(
            scheme_id=scheme.id,
            rule_json=rule_json,
            snippet=raw_text[:500], # Save first 500 chars as snippet reference
            parser_confidence=confidence,
            verified=False # Mark as False so Admins know to check scraped data
        )
        db.session.add(scheme_rule)
        count += 1

    # 4. Commit all changes
    db.session.commit()
    print(f"--- Successfully Inserted {count} Schemes from Scraper ---")